{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Machine Learning - Feature Selection\n",
    "Notes and examples from Udacity's Introduction to Machine Learning course.\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "[Wiki](https://en.wikipedia.org/wiki/Feature_selection) page on feature selection in machine learning \n",
    "\n",
    "[SciKit Learn](http://scikit-learn.org/stable/modules/feature_selection.html) page on feature selection\n",
    "\n",
    "In machine learning features are extremely important, thus which ones we decide to include or not include is also critical.  Feature selection can involve both creating and eliminating variables:\n",
    "\n",
    "1. Creation of features using human intuition can allow for better insight and more powerful analysis.  However, caution must be exercised because if done incorrectly, large amounts of error or completely incorrect data can be introduced to the problem.  Be aware of 100% accuracy as a possible warning sign of poor feature creation.\n",
    "2. Elimination of features can also be done.  This may be necessary because the feature is noisy, it is causing over fitting, the feature is strongly/highly correlated with an already present feature, or simply there is too much data and is causing training/testing to be slow.\n",
    "\n",
    "Regardless, it important to know that:\n",
    "\n",
    "\n",
    "$$ Features \\neq Information $$\n",
    "\n",
    "Features try to access information while the information provided is based on the quality of those features!  Think quality over quantity!\n",
    "\n",
    "A good mantra to live by is to use only as many features as needed.  The question then becomes, how do we best select the features to use on our models?\n",
    "\n",
    "### Univariate Feature Selection\n",
    "SciKit Learn provides many ways to automatically select features; the majority of them fall under the category of [univariate feature selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection).  As the name implies, this method works by treating each feature as an independent case while determining how much influence it has on the final solution.  There are two very popular methods of univariate feature selection in SciKitLearn:\n",
    "\n",
    "1. [SelectPercentile](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile): Selects the user specified percentage of top most influencial features\n",
    "2. [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest): Removes all but the 'K' best features\n",
    "\n",
    "### Bias and Variance\n",
    "The amount of features and information included within your model also inflicts the dilemma of bias versus variance.  High bias is when the algorithm tends to over simplify the model due to a lack of information and generally leads to a low $R^2$ value and a high error on the training set; this is usually due to a lack of features.  High variance on the other hand is when the model pays too much attention to features leading to a higher error on the test set than on the training set.  High variance usually leads to a model not generalizing well as it is too specific to the information it was trained on.\n",
    "\n",
    "The goal when creating a model is to find the 'sweet spot' between bias and variance by optimizing the number of features used.  This can be accomplished through regularization\n",
    "\n",
    "### Regularization\n",
    "Regularization is a method that automatically penalizes extra features as they are added and will thus set non critical variables to zero through the calculation and assignment of a coefficient.  In regression, one of them most popular methods of regularization is with the [LASSO](http://scikit-learn.org/stable/modules/linear_model.html#lasso) method.  Essentially each feature has a coefficient assigned to it based on its importance to the model; the higher the number, the more important the feature.  In total all coefficients must sum to less than a set parameter meaning that some go to zero, effectively removing those features from the model.  More information can be found on the Wikipedia page [here](https://en.wikipedia.org/wiki/Lasso_(statistics).\n",
    "\n",
    "\n",
    "## Overfitting a DecisionTree Example\n",
    "\n",
    "In this example, a new feature is created from the Enron data and used to train a simple decision tree.  The example shows how using a small training set with a lot of features can easily overfit a model.  Also illustrated is how a feature (in this case the newly created one) has the ability to dominate a regression.\n",
    "\n",
    "#### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the usuals\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "words_file = \"../text_learning/your_word_data.pkl\" \n",
    "authors_file = \"../text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load(open(words_file, \"r\"))\n",
    "authors = pickle.load(open(authors_file, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test then vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data into training and test\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(\n",
    "    word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "#create tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small training\n",
    "### set and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create, fit, and predict with a simple decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The accuracy of the Decision Tree classifier with a min_samples_split of 8 is: ', 0.94766780432309439)\n"
     ]
    }
   ],
   "source": [
    "clfTree = tree.DecisionTreeClassifier(min_samples_split=8)\n",
    "clfTree.fit(features_train, labels_train)\n",
    "pred = clfTree.predict(features_test)\n",
    "\n",
    "print('The accuracy of the Decision Tree classifier with a min_samples_split of 8 is: ',\n",
    "      accuracy_score(labels_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that a very simple and basic decision tree is providing a much higher than expected accuracy score; a tell tale sign of overfitting.\n",
    "\n",
    "Lets look at the feature importances and print those that are above 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764705882353 33614\n"
     ]
    }
   ],
   "source": [
    "featImp = clfTree.feature_importances_\n",
    "for x in range(len(featImp)):\n",
    "    if featImp[x] > 0.2:\n",
    "        print featImp[x], x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one feature is above a value of 0.2, meaning that it is the feature mostly controlling the solution.  Lets find out what word that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'sshacklensf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[33614]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
